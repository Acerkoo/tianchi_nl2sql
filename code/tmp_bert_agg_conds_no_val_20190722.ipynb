{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0807 07:50:25.567746 140191956285248 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:10: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import multi_gpu_model\n",
    "from keras_bert import load_vocabulary, load_trained_model_from_checkpoint, Tokenizer, get_checkpoint_paths\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nl2sql.utils import read_data, read_tables, SQL, MultiSentenceTokenizer, Query\n",
    "from nl2sql.utils import SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MULTI_GPUS = True\n",
    "\n",
    "train_table_file = '../data/train/train.tables.json'\n",
    "train_data_file = '../data/train/train.json'\n",
    "\n",
    "val_table_file = '../data/val/val.tables.json'\n",
    "val_data_file = '../data/val/val.json'\n",
    "\n",
    "test_table_file = '../data/test/test.tables.json'\n",
    "test_data_file = '../data/test/test.json'\n",
    "\n",
    "bert_model_path = '../model/chinese_wwm_L-12_H-768_A-12'\n",
    "paths = get_checkpoint_paths(bert_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tables = read_tables(train_table_file)\n",
    "train_data = read_data(train_data_file, train_tables)\n",
    "\n",
    "val_tables = read_tables(val_table_file)\n",
    "val_data = read_data(val_data_file, val_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = load_vocabulary(paths.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryTokenizer(MultiSentenceTokenizer):\n",
    "    col_type_token_dict = {'text': '[unused11]', 'real': '[unused12]'}\n",
    "    \n",
    "    def tokenize(self, query: Query):\n",
    "        question_tokens = [self._token_cls] + self._tokenize(query.question.text)\n",
    "        header_tokens = []\n",
    "\n",
    "        for col_name, col_type in query.table.header:\n",
    "            col_type_token = self.col_type_token_dict[col_type]\n",
    "            col_name_tokens = self._tokenize(col_name)\n",
    "            header_tokens.append([col_type_token] + col_name_tokens)\n",
    "        all_tokens = [question_tokens] + header_tokens\n",
    "        return self._pack(*all_tokens)\n",
    "    \n",
    "    def encode(self, query:Query):\n",
    "        tokens, tokens_lens = self.tokenize(query)\n",
    "        token_ids = self._convert_tokens_to_ids(tokens)\n",
    "        segment_ids = [0] * len(token_ids)\n",
    "        header_indices = np.cumsum(tokens_lens)\n",
    "        return token_ids, segment_ids, header_indices[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tokenizer = QueryTokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryTokenizer\n",
      "Input Question: 二零一九年第四周大黄蜂和密室逃生这两部影片的票房总占比是多少呀\n",
      "Input Header: 影片名称(text) | 周票房（万）(real) | 票房占比（%）(real) | 场均人次(real)\n",
      "Output Tokens: [CLS] 二 零 一 九 年 第 四 周 大 黄 蜂 和 密 室 逃 生 这 两 部 影 片 的 票 房 总 占 比 是 多 少 呀 [SEP] [unused11] 影 片 名 称 [SEP] [unused12] 周 票 房 （ 万 ） [SEP] [unused12] 票 房 占 比 （ % ） [SEP] [unused12] 场 均 人 次 [SEP]\n",
      "Output Encoded: ([101, 753, 7439, 671, 736, 2399, 5018, 1724, 1453, 1920, 7942, 6044, 1469, 2166, 2147, 6845, 4495, 6821, 697, 6956, 2512, 4275, 4638, 4873, 2791, 2600, 1304, 3683, 3221, 1914, 2208, 1435, 102, 11, 2512, 4275, 1399, 4917, 102, 12, 1453, 4873, 2791, 8020, 674, 8021, 102, 12, 4873, 2791, 1304, 3683, 8020, 110, 8021, 102, 12, 1767, 1772, 782, 3613, 102], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], array([33, 39, 47, 56]))\n"
     ]
    }
   ],
   "source": [
    "sample_query = train_data[0]\n",
    "print('QueryTokenizer')\n",
    "print('Input Question: {}'.format(sample_query.question))\n",
    "print('Input Header: {}'.format(sample_query.table.header))\n",
    "print('Output Tokens: {}'.format(' '.join(query_tokenizer.tokenize(sample_query)[0])))\n",
    "print('Output Encoded: {}'.format(query_tokenizer.encode(sample_query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqlLabelEncoder:\n",
    "    \n",
    "    def encode(self, sql: SQL, num_cols):\n",
    "        cond_conn_op_label = sql.cond_conn_op\n",
    "        \n",
    "        sel_agg_label = np.ones(num_cols, dtype='int32') * len(SQL.agg_sql_dict)\n",
    "        for col_id, agg_op in zip(sql.sel, sql.agg):\n",
    "            if col_id < num_cols:\n",
    "                sel_agg_label[col_id] = agg_op\n",
    "            \n",
    "        cond_op_label = np.ones(num_cols, dtype='int32') * len(SQL.op_sql_dict)\n",
    "        for col_id, cond_op, _ in sql.conds:\n",
    "            if col_id < num_cols:\n",
    "                cond_op_label[col_id] = cond_op\n",
    "            \n",
    "        return cond_conn_op_label, sel_agg_label, cond_op_label\n",
    "    \n",
    "    def decode(self, cond_conn_op_label, sel_agg_label, cond_op_label):\n",
    "        cond_conn_op = int(cond_conn_op_label)\n",
    "        sel, agg, conds = [], [], []\n",
    "\n",
    "        for col_id, (agg_op, cond_op) in enumerate(zip(sel_agg_label, cond_op_label)):\n",
    "            if agg_op < len(SQL.agg_sql_dict):\n",
    "                sel.append(col_id)\n",
    "                agg.append(int(agg_op))\n",
    "            if cond_op < len(SQL.op_sql_dict):\n",
    "                conds.append([col_id, int(cond_op)])\n",
    "        return {\n",
    "            'sel': sel,\n",
    "            'agg': agg,\n",
    "            'cond_conn_op': cond_conn_op,\n",
    "            'conds': conds\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = SqlLabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sel': [2], 'agg': [5], 'cond_conn_op': 2, 'conds': [[0, 2]]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.decode(*label_encoder.encode(sample_query.sql, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSequence(Sequence):\n",
    "    def __init__(self, data, tokenizer, label_encoder, is_train=True, max_len=160, batch_size=32, shuffle=True, global_indices=None):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.shuffle = shuffle\n",
    "        self.is_train = is_train\n",
    "        self.max_len = max_len\n",
    "        if global_indices is None:\n",
    "            self._global_indices = np.arange(len(data))\n",
    "        else:\n",
    "            self._global_indices = global_indices\n",
    "        if shuffle:\n",
    "            np.random.shuffle(self._global_indices)\n",
    "    \n",
    "    def _pad_sequences(self, seqs, max_len=None):\n",
    "        padded = pad_sequences(seqs, maxlen=None, padding='post')\n",
    "        if max_len is not None:\n",
    "            padded = padded[:, :max_len]\n",
    "        return padded\n",
    "    \n",
    "    def __getitem__(self, batch_id):\n",
    "        batch_data_indices = \\\n",
    "            self._global_indices[batch_id * self.batch_size: (batch_id + 1) * self.batch_size]\n",
    "        batch_data = [self.data[i] for i in batch_data_indices]\n",
    "        \n",
    "        TOKEN_IDS, SEGMENT_IDS = [], []\n",
    "        HEADER_IDS, HEADER_MASK = [], []\n",
    "        COND_CONN_OP = []\n",
    "        SEL_AGG = []\n",
    "        COND_OP = []\n",
    "        \n",
    "        for query in batch_data:\n",
    "            question = query.question.text\n",
    "            table = query.table\n",
    "            \n",
    "            token_ids, segment_ids, header_ids = self.tokenizer.encode(query)\n",
    "            header_ids = [hid for hid in header_ids if hid < self.max_len]\n",
    "            header_mask = [1] * len(header_ids)\n",
    "            \n",
    "            TOKEN_IDS.append(token_ids)\n",
    "            SEGMENT_IDS.append(segment_ids)\n",
    "            HEADER_IDS.append(header_ids)\n",
    "            HEADER_MASK.append(header_mask)\n",
    "            \n",
    "            if not self.is_train:\n",
    "                continue\n",
    "            sql = query.sql\n",
    "            \n",
    "            cond_conn_op, sel_agg, cond_op = self.label_encoder.encode(sql, num_cols=len(header_ids))\n",
    "            \n",
    "            COND_CONN_OP.append(cond_conn_op)\n",
    "            SEL_AGG.append(sel_agg)\n",
    "            COND_OP.append(cond_op)\n",
    "            \n",
    "        TOKEN_IDS = self._pad_sequences(TOKEN_IDS, max_len=self.max_len)\n",
    "        SEGMENT_IDS = self._pad_sequences(SEGMENT_IDS, max_len=self.max_len)\n",
    "        HEADER_IDS = self._pad_sequences(HEADER_IDS)\n",
    "        HEADER_MASK = self._pad_sequences(HEADER_MASK)\n",
    "        \n",
    "        inputs = {\n",
    "            'input_token_ids': TOKEN_IDS,\n",
    "            'input_segment_ids': SEGMENT_IDS,\n",
    "            'input_header_ids': HEADER_IDS,\n",
    "            'input_header_mask': HEADER_MASK\n",
    "        }\n",
    "        \n",
    "        if self.is_train:\n",
    "            SEL_AGG = self._pad_sequences(SEL_AGG)\n",
    "            SEL_AGG = np.expand_dims(SEL_AGG, axis=-1)\n",
    "            COND_CONN_OP = np.expand_dims(COND_CONN_OP, axis=-1)\n",
    "            COND_OP = self._pad_sequences(COND_OP)\n",
    "            COND_OP = np.expand_dims(COND_OP, axis=-1)\n",
    "\n",
    "            outputs = {\n",
    "                'output_sel_agg': SEL_AGG,\n",
    "                'output_cond_conn_op': COND_CONN_OP,\n",
    "                'output_cond_op': COND_OP\n",
    "            }\n",
    "            return inputs, outputs\n",
    "        else:\n",
    "            return inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.data) / self.batch_size)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self._global_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = DataSequence(train_data, query_tokenizer, label_encoder, shuffle=False, max_len=160, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_token_ids : (2, 63)\n",
      "input_segment_ids : (2, 63)\n",
      "input_header_ids : (2, 4)\n",
      "input_header_mask : (2, 4)\n",
      "output_sel_agg : (2, 4, 1)\n",
      "output_cond_conn_op : (2, 1)\n",
      "output_cond_op : (2, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "sample_inputs, sample_outputs = train_seq[0]\n",
    "for name, data in sample_inputs.items():\n",
    "    print('{} : {}'.format(name, data.shape))\n",
    "#     print(data)\n",
    "    \n",
    "for name, data in sample_outputs.items():\n",
    "    print('{} : {}'.format(name, data.shape))\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5e-5\n",
    "num_sel_agg = len(SQL.agg_sql_dict) + 1\n",
    "num_cond_op = len(SQL.op_sql_dict) + 1\n",
    "num_cond_conn_op = len(SQL.conn_sql_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 07:50:29.255158 140191956285248 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0807 07:50:29.260613 140191956285248 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4140: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0807 07:50:29.308119 140191956285248 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0807 07:50:29.308856 140191956285248 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0807 07:50:29.316616 140191956285248 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3447: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0807 07:50:29.346048 140191956285248 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4187: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model = load_trained_model_from_checkpoint(paths.config, paths.checkpoint, seq_len=None)\n",
    "for l in bert_model.layers:\n",
    "    l.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_gather(x):\n",
    "    seq, idxs = x\n",
    "    idxs = K.cast(idxs, 'int32')\n",
    "    return K.tf.batch_gather(seq, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 07:50:57.445908 140191956285248 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n"
     ]
    }
   ],
   "source": [
    "inp_token_ids = Input(shape=(None,), name='input_token_ids', dtype='int32')\n",
    "inp_segment_ids = Input(shape=(None,), name='input_segment_ids', dtype='int32')\n",
    "inp_header_ids = Input(shape=(None,), name='input_header_ids', dtype='int32')\n",
    "inp_header_mask = Input(shape=(None, ), name='input_header_mask')\n",
    "\n",
    "x = bert_model([inp_token_ids, inp_segment_ids]) # (None, seq_len, 768)\n",
    "\n",
    "# predict cond_conn_op\n",
    "x_for_cond_conn_op = Lambda(lambda x: x[:, 0])(x) # (None, 768)\n",
    "p_cond_conn_op = Dense(num_cond_conn_op, activation='softmax', name='output_cond_conn_op')(x_for_cond_conn_op)\n",
    "\n",
    "# predict sel_agg\n",
    "x_for_header = Lambda(seq_gather, name='header_seq_gather')([x, inp_header_ids]) # (None, h_len, 768)\n",
    "header_mask = Lambda(lambda x: K.expand_dims(x, axis=-1))(inp_header_mask) # (None, h_len, 1)\n",
    "x_for_header = Multiply()([x_for_header, header_mask])\n",
    "x_for_header = Masking()(x_for_header)\n",
    "\n",
    "p_sel_agg = Dense(num_sel_agg, activation='softmax', name='output_sel_agg')(x_for_header)\n",
    "p_cond_op = Dense(num_cond_op, activation='softmax', name='output_cond_op')(x_for_header)\n",
    "\n",
    "model = Model(\n",
    "    [inp_token_ids, inp_segment_ids, inp_header_ids, inp_header_mask],\n",
    "    [p_cond_conn_op, p_sel_agg, p_cond_op]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token_ids (InputLayer)    (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_segment_ids (InputLayer)  (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, None, 768)    101677056   input_token_ids[0][0]            \n",
      "                                                                 input_segment_ids[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "input_header_ids (InputLayer)   (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_header_mask (InputLayer)  (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "header_seq_gather (Lambda)      (None, None, 768)    0           model_2[1][0]                    \n",
      "                                                                 input_header_ids[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, None, 1)      0           input_header_mask[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 768)    0           header_seq_gather[0][0]          \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 768)          0           model_2[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 768)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "output_cond_conn_op (Dense)     (None, 3)            2307        lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output_sel_agg (Dense)          (None, None, 7)      5383        masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output_cond_op (Dense)          (None, None, 5)      3845        masking_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 101,688,591\n",
      "Trainable params: 101,688,591\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using multi-gpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0807 07:52:17.995884 140191956285248 deprecation_wrapper.py:118] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "USE_MULTI_GPUS = True\n",
    "if USE_MULTI_GPUS:\n",
    "    print('using multi-gpus')\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "    \n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=Adam(learning_rate),\n",
    "    metrics={\n",
    "        'output_cond_conn_op': 'accuracy'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataseq = DataSequence(\n",
    "    data=train_data,\n",
    "    tokenizer=query_tokenizer,\n",
    "    label_encoder=label_encoder,\n",
    "    is_train=True, \n",
    "    max_len=160, \n",
    "    batch_size=2\n",
    ")\n",
    "\n",
    "val_dataseq = DataSequence(\n",
    "    data=val_data, \n",
    "    tokenizer=query_tokenizer,\n",
    "    label_encoder=label_encoder,\n",
    "    is_train=True, \n",
    "    max_len=160, \n",
    "    shuffle=False,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, header_lens, label_encoder):\n",
    "    preds_cond_conn_op = np.argmax(preds_cond_conn_op, axis=-1)\n",
    "    preds_sel_agg = np.argmax(preds_sel_agg, axis=-1)\n",
    "    preds_cond_op = np.argmax(preds_cond_op, axis=-1)\n",
    "\n",
    "    sqls = []\n",
    "    \n",
    "    for cond_conn_op, sel_agg, cond_op, header_len in zip(preds_cond_conn_op, \n",
    "                                                          preds_sel_agg, \n",
    "                                                          preds_cond_op, \n",
    "                                                          header_lens):\n",
    "        sql = label_encoder.decode(cond_conn_op, sel_agg, cond_op)\n",
    "        sql['conds'] = [cond for cond in sql['conds'] if cond[0] < header_len]\n",
    "        sel = []\n",
    "        agg = []\n",
    "        for col_id, agg_type in zip(sql['sel'], sql['agg']):\n",
    "            if col_id < header_len:\n",
    "                sel.append(col_id)\n",
    "                agg.append(agg_type)\n",
    "        sql['sel'] = sel\n",
    "        sql['agg'] = agg\n",
    "        sqls.append(sql)\n",
    "    return sqls\n",
    "\n",
    "class EvaluateCallback(Callback):\n",
    "    def __init__(self, val_dataseq):\n",
    "        self.val_dataseq = val_dataseq\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        is_train = self.val_dataseq.is_train\n",
    "        self.val_dataseq.is_train = False\n",
    "        pred_sqls = []\n",
    "        for batch_data in self.val_dataseq:\n",
    "            header_lens = np.sum(batch_data['input_header_mask'], axis=-1)\n",
    "            preds_cond_conn_op, preds_sel_agg, preds_cond_op = self.model.predict_on_batch(batch_data)\n",
    "            sqls = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, \n",
    "                                   header_lens, val_dataseq.label_encoder)\n",
    "            pred_sqls += sqls\n",
    "        conn_correct = 0\n",
    "        agg_correct = 0\n",
    "        conds_correct = 0\n",
    "        conds_col_id_correct = 0\n",
    "        all_correct = 0\n",
    "        num_queries = len(self.val_dataseq.data)\n",
    "        \n",
    "        true_sqls = [query.sql for query in self.val_dataseq.data]\n",
    "        for pred_sql, true_sql in zip(pred_sqls, true_sqls):\n",
    "            n_correct = 0\n",
    "            if pred_sql['cond_conn_op'] == true_sql.cond_conn_op:\n",
    "                conn_correct += 1\n",
    "                n_correct += 1\n",
    "            \n",
    "            pred_aggs = set(zip(pred_sql['sel'], pred_sql['agg']))\n",
    "            true_aggs = set(zip(true_sql.sel, true_sql.agg))\n",
    "            if pred_aggs == true_aggs:\n",
    "                agg_correct += 1\n",
    "                n_correct += 1\n",
    "\n",
    "            pred_conds = set([(cond[0], cond[1]) for cond in pred_sql['conds']])\n",
    "            true_conds = set([(cond[0], cond[1]) for cond in true_sql.conds])\n",
    "\n",
    "            if pred_conds == true_conds:\n",
    "                conds_correct += 1\n",
    "                n_correct += 1\n",
    "   \n",
    "            pred_conds_col_ids = set([cond[0] for cond in pred_sql['conds']])\n",
    "            true_conds_col_ids = set([cond[0] for cond in true_sql['conds']])\n",
    "            if pred_conds_col_ids == true_conds_col_ids:\n",
    "                conds_col_id_correct += 1\n",
    "            \n",
    "            if n_correct == 3:\n",
    "                all_correct += 1\n",
    "\n",
    "        print('conn_acc: {}'.format(conn_correct / num_queries))\n",
    "        print('agg_acc: {}'.format(agg_correct / num_queries))\n",
    "        print('conds_acc: {}'.format(conds_correct / num_queries))\n",
    "        print('conds_col_id_acc: {}'.format(conds_col_id_correct / num_queries))\n",
    "        print('total_acc: {}'.format(all_correct / num_queries))\n",
    "        logs['val_tot_acc'] = all_correct / num_queries\n",
    "        self.val_dataseq.is_train = is_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(Callback):\n",
    "    def __init__(self, init_lr, min_lr):\n",
    "        self.passed = 0\n",
    "        self.init_lr = init_lr\n",
    "        self.min_lr = min_lr\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if self.passed < self.params['steps']:\n",
    "            lr = (self.passed + 1) / self.params['steps'] * self.init_lr\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            self.passed += 1\n",
    "        elif self.params['steps'] <= self.passed < self.params['steps'] * 2:\n",
    "            lr = (2 - (self.passed + 1) / self.params['steps']) * (self.init_lr - self.min_lr)\n",
    "            lr += self.min_lr\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            self.passed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    LearningRateScheduler(learning_rate, min_lr=1e-5),\n",
    "    EvaluateCallback(val_dataseq),\n",
    "    ModelCheckpoint(\"output/bert_all.{epoch:02d}-{val_tot_acc:.3f}.hd5\", monitor='val_tot_acc', mode='max', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/13\n",
      "649/649 [==============================] - 418s 643ms/step - loss: 0.0178 - output_cond_conn_op_loss: 0.0049 - output_sel_agg_loss: 0.0056 - output_cond_op_loss: 0.0074 - output_cond_conn_op_acc: 0.9986\n",
      "conn_acc: 0.95950864422202\n",
      "agg_acc: 0.9499545040946314\n",
      "conds_acc: 0.8971792538671519\n",
      "conds_col_id_acc: 0.9069608735213831\n",
      "total_acc: 0.8416742493175614\n",
      "Epoch 12/13\n",
      "649/649 [==============================] - 448s 690ms/step - loss: 0.0157 - output_cond_conn_op_loss: 0.0035 - output_sel_agg_loss: 0.0055 - output_cond_op_loss: 0.0067 - output_cond_conn_op_acc: 0.9991\n",
      "conn_acc: 0.9638307552320291\n",
      "agg_acc: 0.9488171064604186\n",
      "conds_acc: 0.905368516833485\n",
      "conds_col_id_acc: 0.9156050955414012\n",
      "total_acc: 0.8519108280254777\n",
      "Epoch 13/13\n",
      "649/649 [==============================] - 455s 702ms/step - loss: 0.0152 - output_cond_conn_op_loss: 0.0048 - output_sel_agg_loss: 0.0045 - output_cond_op_loss: 0.0059 - output_cond_conn_op_acc: 0.9982\n",
      "conn_acc: 0.9597361237488626\n",
      "agg_acc: 0.9483621474067334\n",
      "conds_acc: 0.8971792538671519\n",
      "conds_col_id_acc: 0.9074158325750682\n",
      "total_acc: 0.8434940855323021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68cbddc898>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_dataseq, epochs=13, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../model/bert_all.12-0.852.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790caf11bc674f25ba5ab2a5b210ce84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=69), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_dataseq.is_train = False\n",
    "pred_sqls = []\n",
    "\n",
    "for batch_data in tqdm(val_dataseq):\n",
    "    header_lens = np.sum(batch_data['input_header_mask'], axis=-1)\n",
    "    preds_cond_conn_op, preds_sel_agg, preds_cond_op = model.predict_on_batch(batch_data)\n",
    "    sqls = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, \n",
    "                           header_lens, val_dataseq.label_encoder)\n",
    "    pred_sqls += sqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/bert_noval_outputs_190722.json', 'w') as f:\n",
    "    for sql in pred_sqls:\n",
    "        f.write(json.dumps(sql, ensure_ascii=True) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tables = read_tables(test_table_file)\n",
    "test_data = read_data(test_data_file, test_tables)\n",
    "\n",
    "test_dataseq = DataSequence(\n",
    "    data=test_data, \n",
    "    tokenizer=query_tokenizer,\n",
    "    label_encoder=label_encoder,\n",
    "    is_train=False, \n",
    "    max_len=160, \n",
    "    shuffle=False,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5cfc530d8b4b899b117b9b1022b21d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_sqls = []\n",
    "\n",
    "for batch_data in tqdm(test_dataseq):\n",
    "    header_lens = np.sum(batch_data['input_header_mask'], axis=-1)\n",
    "    preds_cond_conn_op, preds_sel_agg, preds_cond_op = model.predict_on_batch(batch_data)\n",
    "    sqls = outputs_to_sqls(preds_cond_conn_op, preds_sel_agg, preds_cond_op, \n",
    "                           header_lens, test_dataseq.label_encoder)\n",
    "    pred_sqls += sqls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4086"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_sqls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/test_bert_noval_outputs_190722.json', 'w') as f:\n",
    "    for sql in pred_sqls:\n",
    "        f.write(json.dumps(sql, ensure_ascii=True) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
